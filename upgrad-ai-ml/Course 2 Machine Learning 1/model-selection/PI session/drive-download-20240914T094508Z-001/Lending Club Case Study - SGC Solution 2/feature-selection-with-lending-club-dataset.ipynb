{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection with Lending Club Dataset\n",
    "\n",
    "<!-- introduction - what is FS and why do we need to do it -->\n",
    "One of the most relevant topics in machine learning is identifying which features or attributes to use in your models. In most real world problems, data often has many features but only few are related to the target we are interested in. Feature selection is the process of selecting a subset of features which are then used in model construction.\n",
    "\n",
    "We are going to look at data from a popular lending marketplace Lending Club.  Lending club has been transforming the banking system with it's peer-to-peer lending model, where it acts as a bridge between the investor and the borrowers. In this end to end example we're going to be examining different attributes that lending club collects from borrowers and how they influence the interest rates. The dataset contains loan applications from 2007 through 2018 with current loan status and payment information.\n",
    "\n",
    "Our example dataset has 150 columns containing both categorical and continuous attributes. With feature selection, we are going to create a model with as good or better accuracy while requiring lesser data. Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# models\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoLars, LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, RFE\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from multiprocessing import Pool\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection goes hand in hand with the initial steps of cleaning and preparing your data before the training process.\n",
    "\n",
    "\n",
    "Checking for missing values is a good first step in preparing the data. We have chosen to remove columns where more than 65% of the values are empty. By working on the dataset slice-by-slice, we have converted the categorical features to one-hot encoding. Next, we will jump into various feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/clq6twsx7wz60w7pwrf8977h0000gn/T/ipykernel_37153/340379443.py:2: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  loan_df = pd.read_csv(\"loan.csv\")\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset\n",
    "loan_df = pd.read_csv(\"loan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with more than 60% missing values - \n",
      "mo_sin_old_il_acct                100.000000\n",
      "mo_sin_old_rev_tl_op              100.000000\n",
      "mo_sin_rcnt_tl                    100.000000\n",
      "mort_acc                          100.000000\n",
      "mths_since_recent_bc              100.000000\n",
      "mths_since_recent_bc_dlq          100.000000\n",
      "mths_since_recent_inq             100.000000\n",
      "mths_since_recent_revol_delinq    100.000000\n",
      "num_accts_ever_120_pd             100.000000\n",
      "num_actv_bc_tl                    100.000000\n",
      "num_actv_rev_tl                   100.000000\n",
      "num_bc_sats                       100.000000\n",
      "num_bc_tl                         100.000000\n",
      "num_il_tl                         100.000000\n",
      "num_op_rev_tl                     100.000000\n",
      "num_rev_accts                     100.000000\n",
      "num_rev_tl_bal_gt_0               100.000000\n",
      "num_sats                          100.000000\n",
      "num_tl_120dpd_2m                  100.000000\n",
      "num_tl_30dpd                      100.000000\n",
      "num_tl_90g_dpd_24m                100.000000\n",
      "num_tl_op_past_12m                100.000000\n",
      "pct_tl_nvr_dlq                    100.000000\n",
      "percent_bc_gt_75                  100.000000\n",
      "tot_hi_cred_lim                   100.000000\n",
      "total_bal_ex_mort                 100.000000\n",
      "total_bc_limit                    100.000000\n",
      "mo_sin_rcnt_rev_tl_op             100.000000\n",
      "total_il_high_credit_limit        100.000000\n",
      "total_bal_il                      100.000000\n",
      "bc_util                           100.000000\n",
      "annual_inc_joint                  100.000000\n",
      "dti_joint                         100.000000\n",
      "verification_status_joint         100.000000\n",
      "tot_coll_amt                      100.000000\n",
      "tot_cur_bal                       100.000000\n",
      "open_acc_6m                       100.000000\n",
      "open_il_6m                        100.000000\n",
      "open_il_12m                       100.000000\n",
      "open_il_24m                       100.000000\n",
      "mths_since_rcnt_il                100.000000\n",
      "il_util                           100.000000\n",
      "open_rv_12m                       100.000000\n",
      "open_rv_24m                       100.000000\n",
      "max_bal_bc                        100.000000\n",
      "all_util                          100.000000\n",
      "total_rev_hi_lim                  100.000000\n",
      "inq_fi                            100.000000\n",
      "total_cu_tl                       100.000000\n",
      "inq_last_12m                      100.000000\n",
      "acc_open_past_24mths              100.000000\n",
      "avg_cur_bal                       100.000000\n",
      "bc_open_to_buy                    100.000000\n",
      "mths_since_last_major_derog       100.000000\n",
      "next_pymnt_d                       97.129693\n",
      "mths_since_last_record             92.985372\n",
      "mths_since_last_delinq             64.662487\n",
      "dtype: float64\n",
      "(39717, 54)\n"
     ]
    }
   ],
   "source": [
    "# looking at missing values\n",
    "# percentage of null values in each of the columns \n",
    "missing_val = loan_df.isna().sum()/len(loan_df)*100\n",
    "print(\"Columns with more than 60% missing values - \")\n",
    "print(missing_val[missing_val > 60].sort_values(ascending=False))\n",
    "# since we cannot fill in values for these columns we're going to drop these\n",
    "loan_df = loan_df.loc[:, loan_df.isnull().mean() < .35]\n",
    "print(loan_df.shape)\n",
    "# got rid of ~50 cols with that threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dropping arbitrary applicant input\n",
    "loan_df.drop(columns = ['emp_title', 'title', 'initial_list_status'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['term', 'grade', 'sub_grade', 'emp_length', 'home_ownership',\n",
       "       'verification_status', 'issue_d', 'loan_status', 'pymnt_plan',\n",
       "       'purpose', 'zip_code', 'addr_state', 'earliest_cr_line', 'last_pymnt_d',\n",
       "       'last_credit_pull_d', 'application_type', 'hardship_flag',\n",
       "       'disbursement_method', 'debt_settlement_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at cols of object type - convert dates to duration, emp_length to int, encode grade and subgrade, \n",
    "loan_df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.284932</td>\n",
       "      <td>0.605479</td>\n",
       "      <td>0.435616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.128767</td>\n",
       "      <td>0.605479</td>\n",
       "      <td>0.435616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.279452</td>\n",
       "      <td>0.605479</td>\n",
       "      <td>0.435616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.443836</td>\n",
       "      <td>0.605479</td>\n",
       "      <td>0.435616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.616438</td>\n",
       "      <td>0.605479</td>\n",
       "      <td>0.435616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   earliest_cr_line   issue_d  last_credit_pull_d\n",
       "0         18.284932  0.605479            0.435616\n",
       "1         32.128767  0.605479            0.435616\n",
       "2          8.279452  0.605479            0.435616\n",
       "3         13.443836  0.605479            0.435616\n",
       "4         18.616438  0.605479            0.435616"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all dates like issue_d, earliest_cr_line, last_credit_pull_d will be converted to duration\n",
    "dttoday = datetime.now().strftime('%Y-%m-%d')\n",
    "loan_df.fillna({'earliest_cr_line':dttoday, 'issue_d':dttoday, 'last_credit_pull_d':dttoday}, inplace=True)\n",
    "loan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']] = loan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']].apply(pd.to_datetime)\n",
    "\n",
    "loan_df['earliest_cr_line'] = loan_df['earliest_cr_line'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))/-365)\n",
    "loan_df['last_credit_pull_d'] = loan_df['last_credit_pull_d'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))/-365)\n",
    "loan_df['issue_d'] = loan_df['issue_d'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))/-365)\n",
    "\n",
    "loan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# convert zip from string to int\n",
    "loan_df['zip_code'] = loan_df['zip_code'].apply(lambda x: str(x)[:3])\n",
    "loan_df['zip_code'] = pd.to_numeric(loan_df['zip_code'], errors='coerce')\n",
    "loan_df['zip_code'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10+ years    748005\n",
       "2 years      203677\n",
       "< 1 year     189988\n",
       "3 years      180753\n",
       "1 year       148403\n",
       "0 years      146907\n",
       "5 years      139698\n",
       "4 years      136605\n",
       "6 years      102628\n",
       "7 years       92695\n",
       "8 years       91914\n",
       "9 years       79395\n",
       "Name: emp_length, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting emp_years to int\n",
    "loan_df.fillna({'emp_length':'0 years', 'term':'Unknown', 'home_ownership':'Unknown',\n",
    "                'verification_status':'Unknown', 'loan_status':'Unknown', 'purpose':'Unknown',\n",
    "                'addr_state':'Unknown', 'application_type':'Unknown', 'disbursement_method':'Unknown'}, inplace=True)\n",
    "loan_df['emp_length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting emp_length to float\n",
    "dict_emp_length = {'10+ years':10, '6 years':6, '4 years':4, '< 1 year':0.5, '2 years':2,\n",
    "       '9 years':9, '0 years':0, '5 years':5, '3 years':3, '7 years':7, '1 year':1,\n",
    "       '8 years':8}\n",
    "loan_df['emp_length'].replace(dict_emp_length, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping arbitrary applicant input, post loan attributes\n",
    "to_drop = ['pymnt_plan', 'total_pymnt',\n",
    "           'total_pymnt_inv','last_pymnt_d', 'last_pymnt_amnt', 'out_prncp',\n",
    "           'out_prncp_inv', 'total_rec_prncp', 'hardship_flag', 'collection_recovery_fee',\n",
    "           'collections_12_mths_ex_med', 'debt_settlement_flag','policy_code', 'grade', 'sub_grade']\n",
    "loan_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2260668 entries, 0 to 2260667\n",
      "Columns: 150 entries, loan_amnt to disbursement_method_DirectPay\n",
      "dtypes: float64(58), int64(3), uint8(89)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "loan_dummy_df = pd.get_dummies(loan_df.drop(columns=to_drop, axis=1))\n",
    "loan_dummy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2260668, 150)\n"
     ]
    }
   ],
   "source": [
    "# save new dataset for checkpointing\n",
    "# loan_dummy_df.to_csv(\"loan_dummy.csv\", index=False)\n",
    "print(loan_dummy_df.shape)\n",
    "\n",
    "# splitting into test and train after treating variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(loan_dummy_df, loan_df['int_rate'], \n",
    "                                                    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "X_train.drop(columns=['int_rate'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['int_rate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "# number of columns in the cleaned dataset\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track of models\n",
    "\n",
    "With a large number of variables, hyperparameters and different types of models, it is easy to lose track of the progress we have made. To help with this problem we are going to be using the [Verta.AI](https://www.verta.ai/)'s model management platform to keep track of models and graphs that we build thoughout the process. Setting up Verta is simple and straightforward and more information can be found [here](https://verta.readme.io/docs/getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successfully established\n",
      "created new Project: \n"
     ]
    }
   ],
   "source": [
    "# app settings\n",
    "from verta import Client\n",
    "\n",
    "# setting up\n",
    "HOST = \"app.verta.ai\"\n",
    "EMAIL = \"your-email@gmail.com\"\n",
    "DEV_KEY= \"your-dev-key\"\n",
    "PROJECT_NAME = \"Feature-Selection\"\n",
    "\n",
    "client = Client(host=HOST,\n",
    "                email=EMAIL, \n",
    "                dev_key=DEV_KEY,\n",
    "                ignore_conn_err=True)\n",
    "proj = client.set_project(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting out hands dirty\n",
    "\n",
    "Now that our cleaned datset is in place and we have our model logging set up, we will look at 5 popular methods across these categories in detail:\n",
    "\n",
    "1. Using domain knowledge\n",
    "2. Removing highly correlated features\n",
    "3. Univariate feature selection\n",
    "4. Recursive feature elimination\n",
    "5. Embedded methods\n",
    "\n",
    "For each method, we are going to use our smaller feature set which is created to train 3 different types of models -\n",
    "1. elastic net model (baseline)\n",
    "2. random forests\n",
    "3. gradient boosting regressor\n",
    "\n",
    "We have chosen these models as they generally work for large datasets, prevent overfitting and are straightforward to understand. However of the three models, the gradient boosting regressor takes the longest to train and requires hyperparameter tuning to be done for each feature set. If compute power is a constraint, skipping this model might be in your interest. \n",
    "\n",
    "**For each feature selection method, the modeling cells have been commented out to reduce the runtime of this kernel, to run any model, feel free to uncomment those lines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models + Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating and logging mean absolute error and root mean squared error for each model\n",
    "def get_metrics(X_train, y_train, X_test, y_test, model, run=None):\n",
    "    \n",
    "    print(\"Getting metrics for model\")\n",
    "    # train prediction\n",
    "    train_preds = model.predict(X_train)\n",
    "    # test prediction\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate the absolute errors\n",
    "    train_errors = abs(train_preds - y_train)\n",
    "    test_errors = abs(test_preds - y_test)\n",
    "    \n",
    "    # mean absolute error (mae)\n",
    "    train_mae = round(np.mean(train_errors), 4)\n",
    "    test_mae = round(np.mean(test_errors), 4)\n",
    "    print(\"train mae: \", train_mae, \" test mae: \", test_mae)\n",
    "\n",
    "    # root mean sq error\n",
    "    train_rmse = np.sqrt(metrics.mean_squared_error(y_train, train_preds))\n",
    "    test_rmse = np.sqrt(metrics.mean_squared_error(y_test, test_preds))\n",
    "    print(\"train rmse: \", train_mae, \" test rmse: \", test_mae)\n",
    "\n",
    "    \n",
    "    # log values only if run is provided    \n",
    "    if run:\n",
    "        run.log_metric(\"train_mae\", train_mae)\n",
    "        run.log_metric(\"train_rmse\", train_rmse)\n",
    "        run.log_metric(\"test_mae\", test_mae)\n",
    "        run.log_metric(\"test_rmse\", test_rmse)\n",
    "    \n",
    "    return train_mae, test_mae, train_rmse, test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic net model - this logs all hyperparameters, features and metrics\n",
    "def elastic_net_model(tags, feature_set, drop = True,  **params):\n",
    "    # elastic net regression\n",
    "    \n",
    "    if drop:\n",
    "        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n",
    "        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n",
    "    else:\n",
    "        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "    \n",
    "    # grid search\n",
    "    param_grid = {'l1_ratio': [.1, .5, .7, .85, .98, 1]} \n",
    "    en_regr = GridSearchCV(ElasticNet(), param_grid, \n",
    "                           scoring='neg_mean_absolute_error',\n",
    "                           n_jobs=-1, refit=True, cv=3, verbose=20, return_train_score=True)\n",
    "    \n",
    "    en_regr.fit(X_train_new, y_train)\n",
    "    results = pd.DataFrame(en_regr.cv_results_)\n",
    "    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n",
    "\n",
    "    for _, run_result in results.iterrows():\n",
    "        run = client.set_experiment_run()\n",
    "        run.log_tags(tags)\n",
    "        \n",
    "        if drop:\n",
    "            run.log_attribute(\"removed_features\", removed_features)\n",
    "        else:\n",
    "            run.log_attribute(\"feature_set\", feature_set)\n",
    "\n",
    "        # log hyperparameters\n",
    "        run.log_hyperparameters(run_result['params'])\n",
    "        \n",
    "        # run statistics    \n",
    "        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n",
    "        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n",
    "        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n",
    "\n",
    "        # log summary stats of validation\n",
    "        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n",
    "        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n",
    "    \n",
    "        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n",
    "        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n",
    "  \n",
    "\n",
    "    # log best model\n",
    "    run_baseline_en = client.set_experiment_run()\n",
    "    run_baseline_en.log_hyperparameters(en_regr.best_params_)\n",
    "    run_baseline_en.log_model(key=\"elastic_net_model\", model=en_regr)\n",
    "    run_baseline_en.log_tags(tags+['best model'])\n",
    "    \n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "                                                             y_train = y_train,\n",
    "                                                             X_test = X_test_new,\n",
    "                                                             y_test = y_test,\n",
    "                                                             model = en_regr,\n",
    "                                                             run = run_baseline_en)\n",
    "    return en_regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model - this logs all hyperparameters, features and metrics\n",
    "def random_forest_model(tags, feature_set, drop = True, **params):\n",
    "    # Random Forest\n",
    "    # creating new train and test sets\n",
    "    # logging features     \n",
    "    if drop:\n",
    "        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n",
    "        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n",
    "    else:\n",
    "        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "    \n",
    "    param_grid = {\n",
    "    'n_estimators':[50],\n",
    "    'max_depth': [4, 9],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    }\n",
    "    \n",
    "    rf = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                      param_grid, n_jobs=4, refit=True, verbose=20, cv=3, \n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      return_train_score=True)\n",
    "    \n",
    "    rf.fit(X_train_new, y_train)\n",
    "    \n",
    "    results = pd.DataFrame(rf.cv_results_)\n",
    "    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n",
    "\n",
    "    for _, run_result in results.iterrows():\n",
    "        run = client.set_experiment_run()\n",
    "        run.log_tags(tags)\n",
    "        \n",
    "        if drop:\n",
    "            run.log_attribute(\"removed_features\", feature_set)\n",
    "        else:\n",
    "            run.log_attribute(\"feature_set\", feature_set)\n",
    "\n",
    "        # log hyperparameters\n",
    "        run.log_hyperparameters(run_result['params'])\n",
    "        \n",
    "        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n",
    "        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n",
    "        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n",
    "\n",
    "        # log summary stats of validation\n",
    "        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n",
    "        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n",
    "    \n",
    "        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n",
    "        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n",
    "\n",
    "    # best model\n",
    "    run_rf = client.set_experiment_run()\n",
    "    run_rf.log_hyperparameters(rf.best_params_)\n",
    "    run_rf.log_model(key=\"model\", model=rf)\n",
    "    run_rf.log_tags(tags+['best model'])\n",
    "\n",
    "    # metrics for models\n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "                                                             y_train = y_train,\n",
    "                                                             X_test = X_test_new,\n",
    "                                                             y_test = y_test,\n",
    "                                                             model = rf,\n",
    "                                                             run = run_rf)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBR model - this logs all hyperparameters, features and metrics\n",
    "def gbr_model(tags, feature_set, drop = True, **params):\n",
    "    # Gradient Boosting Regressor\n",
    "    # creating new train and test sets\n",
    "    if drop:\n",
    "        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n",
    "        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n",
    "    else:\n",
    "        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators':[250],\n",
    "        'learning_rate': [0.1, 0.01,0.05, 0.02],\n",
    "        'max_depth': [4, 6, 9],\n",
    "        'min_samples_leaf': [3, 7, 15]\n",
    "             }\n",
    "\n",
    "    gbr = GridSearchCV(GradientBoostingRegressor(), param_grid, n_jobs=-1, verbose=20, refit=True,\n",
    "                       cv=3, scoring='neg_mean_absolute_error',\n",
    "                       return_train_score=True)\n",
    "    \n",
    "    gbr.fit(X_train_new, y_train)\n",
    "    \n",
    "    results = pd.DataFrame(gbr.cv_results_)\n",
    "    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n",
    "\n",
    "    for _, run_result in results.iterrows():\n",
    "        run = client.set_experiment_run()\n",
    "        run.log_tags(tags)\n",
    "        \n",
    "        if drop:\n",
    "            run.log_attribute(\"removed_features\", feature_set)\n",
    "        else:\n",
    "            run.log_attribute(\"feature_set\", feature_set)\n",
    "\n",
    "        # log hyperparameters\n",
    "        run.log_hyperparameters(run_result['params'])\n",
    "        \n",
    "        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n",
    "        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n",
    "        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n",
    "\n",
    "        # log summary stats of validation\n",
    "        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n",
    "        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n",
    "    \n",
    "        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n",
    "        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n",
    "\n",
    "\n",
    "    # log best model\n",
    "    run_gbr = client.set_experiment_run()\n",
    "    run_gbr.log_hyperparameters(gbr.best_params_)\n",
    "    run_gbr.log_model(key=\"model\", model=gbr)\n",
    "    run_gbr.log_tags(tags+['best model'])\n",
    "\n",
    "    # metrics for models\n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "                                                             y_train = y_train,\n",
    "                                                             X_test = X_test_new,\n",
    "                                                             y_test = y_test,\n",
    "                                                             model = gbr,\n",
    "                                                             run = run_gbr)\n",
    "    return gbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using domain knowledge to remove features\n",
    "Getting to know the dataset and how features can be used for the problem can give one a headstart on building a good model. In our case, the dataset comes with a data dictionary which is a good place to start. We are getting rid of columns which are free text provided by the borrower and columns related to incidents after the loan was granted as these are not of use to us.\n",
    "Since we have already removed those columns in the cleaning stage, we can create a model with all the features to see how much worse the model would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dk = client.set_experiment_run()\n",
    "# loan_dummy_dk = pd.get_dummies(loan_df.drop(columns=['grade', 'sub_grade'], axis=1))\n",
    "\n",
    "\n",
    "# # splitting into test and train after treating variables\n",
    "# X_train_dk, X_test_dk, y_train_dk, y_test_dk = train_test_split(loan_dummy_dk, loan_dummy_dk['int_rate'], \n",
    "#                                                     test_size=0.3, random_state=42)\n",
    "\n",
    "# # creating new train and test sets\n",
    "# X_train_dk.drop(columns=['int_rate'], axis=1, inplace=True)\n",
    "# X_test_dk.drop(columns=['int_rate'], axis=1, inplace=True)\n",
    "# # MODEL:\n",
    "# expt = client.set_experiment(\"domain_knowledge\")\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train_dk, y_train)\n",
    "# # best model\n",
    "# run_rf = client.set_experiment_run()\n",
    "# run_rf.log_model(key=\"model\", model=rf)\n",
    "# run_rf.log_tags(['domain_knowledge', 'random_forest', 'no_HPP'])\n",
    "\n",
    "# # metrics for models\n",
    "# train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "#                                                          y_train = y_train,\n",
    "#                                                          X_test = X_test_new,\n",
    "#                                                          y_test = y_test,\n",
    "#                                                          model = rf,\n",
    "#                                                          run = run_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing low variance features\n",
    "\n",
    "In sklearn’s `feature_selection` module we find `VarianceThreshold`. It removes all features whose variance doesn’t meet some threshold. By default it removes features with zero variance or features that have the same value for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Experiment: \n",
      "Found 85 low-variance columns.\n",
      "Low-variance columns ['acc_now_delinq', 'addr_state_AK', 'addr_state_AL', 'addr_state_AR', 'addr_state_AZ', 'addr_state_CA', 'addr_state_CO', 'addr_state_CT', 'addr_state_DC', 'addr_state_DE', 'addr_state_FL', 'addr_state_GA', 'addr_state_HI', 'addr_state_IA', 'addr_state_ID', 'addr_state_IL', 'addr_state_IN', 'addr_state_KS', 'addr_state_KY', 'addr_state_LA', 'addr_state_MA', 'addr_state_MD', 'addr_state_ME', 'addr_state_MI', 'addr_state_MN', 'addr_state_MO', 'addr_state_MS', 'addr_state_MT', 'addr_state_NC', 'addr_state_ND', 'addr_state_NE', 'addr_state_NH', 'addr_state_NJ', 'addr_state_NM', 'addr_state_NV', 'addr_state_NY', 'addr_state_OH', 'addr_state_OK', 'addr_state_OR', 'addr_state_PA', 'addr_state_RI', 'addr_state_SC', 'addr_state_SD', 'addr_state_TN', 'addr_state_TX', 'addr_state_UT', 'addr_state_VA', 'addr_state_VT', 'addr_state_WA', 'addr_state_WI', 'addr_state_WV', 'addr_state_WY', 'application_type_Individual', 'application_type_Joint App', 'chargeoff_within_12_mths', 'disbursement_method_Cash', 'disbursement_method_DirectPay', 'home_ownership_ANY', 'home_ownership_NONE', 'home_ownership_OTHER', 'home_ownership_OWN', 'loan_status_Charged Off', 'loan_status_Default', 'loan_status_Does not meet the credit policy. Status:Charged Off', 'loan_status_Does not meet the credit policy. Status:Fully Paid', 'loan_status_In Grace Period', 'loan_status_Late (16-30 days)', 'loan_status_Late (31-120 days)', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'pub_rec_bankruptcies', 'purpose_car', 'purpose_credit_card', 'purpose_educational', 'purpose_home_improvement', 'purpose_house', 'purpose_major_purchase', 'purpose_medical', 'purpose_moving', 'purpose_other', 'purpose_renewable_energy', 'purpose_small_business', 'purpose_vacation', 'purpose_wedding', 'tax_liens']\n"
     ]
    }
   ],
   "source": [
    "expt_lv = client.set_experiment(\"low_variance\")\n",
    "\n",
    "# get list of all the original df columns\n",
    "all_columns = X_train.columns\n",
    "\n",
    "# instantiate VarianceThreshold object\n",
    "vt = VarianceThreshold(threshold=0.2)\n",
    "\n",
    "# fit vt to data\n",
    "vt.fit(X_train)\n",
    "\n",
    "# get the indices of the features that are being kept\n",
    "feature_indices = vt.get_support(indices=True)\n",
    "\n",
    "# remove low-variance columns from index\n",
    "feature_names = [all_columns[idx]\n",
    "                 for idx, _\n",
    "                 in enumerate(all_columns)\n",
    "                 if idx\n",
    "                 in feature_indices]\n",
    "\n",
    "# get the columns to be removed\n",
    "removed_features = list(np.setdiff1d(all_columns,\n",
    "                                     feature_names))\n",
    "print(\"Found {0} low-variance columns.\"\n",
    "      .format(len(removed_features)))\n",
    "print(\"Low-variance columns\", removed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # elatic net regr\n",
    "# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=removed_features, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # random forest\n",
    "# rf_lv = random_forest_model(tags=['random forest'], feature_set=removed_features, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # GBR\n",
    "# gbr_lv = gbr_model(tags=['gradient boosting regressor'], feature_set=removed_features, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing highly correlated features\n",
    "After examining our dataframe's correlation matrix we drop highly correlated/redundant data to address multicollinearity. When these are not removed it can lead to decreased generalization performance on the test set due to high variance and less model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Experiment: \n",
      "Columns with corr. greater than 0.7 - \n",
      "funded_amnt                    loan_amnt                      0.999759\n",
      "funded_amnt_inv                loan_amnt                      0.999033\n",
      "                               funded_amnt                    0.999332\n",
      "installment                    loan_amnt                      0.945696\n",
      "                               funded_amnt                    0.946034\n",
      "                               funded_amnt_inv                0.945176\n",
      "total_acc                      open_acc                       0.718098\n",
      "total_rev_hi_lim               revol_bal                      0.787114\n",
      "avg_cur_bal                    tot_cur_bal                    0.830753\n",
      "bc_util                        revol_util                     0.799941\n",
      "mo_sin_old_rev_tl_op           earliest_cr_line               0.827485\n",
      "num_actv_rev_tl                num_actv_bc_tl                 0.834312\n",
      "num_bc_sats                    num_actv_bc_tl                 0.841922\n",
      "num_bc_tl                      num_bc_sats                    0.757869\n",
      "num_op_rev_tl                  open_acc                       0.807999\n",
      "                               num_actv_rev_tl                0.815671\n",
      "                               num_bc_sats                    0.771544\n",
      "num_rev_accts                  total_acc                      0.727947\n",
      "                               num_bc_tl                      0.852910\n",
      "                               num_op_rev_tl                  0.812176\n",
      "num_rev_tl_bal_gt_0            num_actv_bc_tl                 0.828406\n",
      "                               num_actv_rev_tl                0.985009\n",
      "                               num_op_rev_tl                  0.819662\n",
      "num_sats                       open_acc                       0.958206\n",
      "                               num_op_rev_tl                  0.842892\n",
      "num_tl_30dpd                   acc_now_delinq                 0.804095\n",
      "num_tl_op_past_12m             acc_open_past_24mths           0.766800\n",
      "percent_bc_gt_75               revol_util                     0.708839\n",
      "                               bc_util                        0.847770\n",
      "tot_hi_cred_lim                tot_cur_bal                    0.976235\n",
      "                               avg_cur_bal                    0.791584\n",
      "total_bc_limit                 total_rev_hi_lim               0.760566\n",
      "                               bc_open_to_buy                 0.845375\n",
      "total_il_high_credit_limit     total_bal_ex_mort              0.877420\n",
      "term_ 60 months                term_ 36 months               -1.000000\n",
      "home_ownership_RENT            home_ownership_MORTGAGE       -0.795775\n",
      "loan_status_Fully Paid         loan_status_Current           -0.766048\n",
      "application_type_Joint App     application_type_Individual   -1.000000\n",
      "disbursement_method_DirectPay  disbursement_method_Cash      -1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "expt = client.set_experiment(\"Correlated features\")\n",
    "\n",
    "# corr matrix\n",
    "cor = X_train.corr()\n",
    "cor.loc[:,:] = np.tril(cor, k=-1) # below main lower triangle of an array\n",
    "cor_stack = cor.stack()\n",
    "print(\"Columns with corr. greater than 0.7 - \")\n",
    "print(cor_stack[(cor_stack > 0.70) | (cor_stack < -0.70)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all cols whose corr is greater then 0.7; generated dummy columns excluded\n",
    "cols_to_drop = ['funded_amnt', 'funded_amnt_inv', 'installment', 'total_acc', 'total_rev_hi_lim','avg_cur_bal',\n",
    "                'bc_util', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_tl', 'num_actv_rev_tl', 'num_bc_sats', \n",
    "                'num_bc_tl', 'num_il_tl', 'num_op_rev_tl','num_rev_accts','num_rev_tl_bal_gt_0',\n",
    "                'num_sats','num_tl_30dpd', 'num_tl_op_past_12m', 'percent_bc_gt_75',\n",
    "               'tax_liens', 'tot_hi_cred_lim', 'total_bc_limit', 'total_il_high_credit_limit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns corr. to target variable -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at columns highly correlated with the target column\n",
    "int_rate_cor = X_train.apply(lambda x: x.corr(y_train))\n",
    "print(\"Columns corr. to target variable -\")\n",
    "int_rate_cor[(int_rate_cor > 0.5) | (int_rate_cor < -0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # elatic net regr\n",
    "# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=cols_to_drop, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # random forest\n",
    "# rf_corr = random_forest_model(tags=['random forest'], feature_set=cols_to_drop, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # GBR\n",
    "# gbr_model(tags=['gradient boosting regressor'], feature_set=cols_to_drop, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection\n",
    "\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. We can use sklearn’s `SelectKBest` to select a number of features to keep. This method uses statistical tests to select features having the highest correlation to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Experiment: \n",
      "K best columns:  ['installment', 'dti', 'inq_last_6mths', 'revol_util', 'total_rec_int', 'recoveries', 'total_rev_hi_lim', 'acc_open_past_24mths', 'bc_open_to_buy', 'bc_util', 'num_tl_op_past_12m', 'percent_bc_gt_75', 'total_bc_limit', 'term_ 36 months', 'term_ 60 months', 'verification_status_Not Verified', 'verification_status_Verified', 'loan_status_Charged Off', 'purpose_credit_card', 'disbursement_method_DirectPay']\n"
     ]
    }
   ],
   "source": [
    "expt = client.set_experiment(\"f_regr\")\n",
    "\n",
    "# for regression use f_regression\n",
    "# k is the number of features selected, here it is set to 20 but can be varied\n",
    "selector_f_reg = SelectKBest(f_regression, k=20).fit(X_train, y_train)\n",
    "\n",
    "selected_cols = [d for d, s in zip(list(X_train.columns), selector_f_reg.get_support()) if s]\n",
    "print(\"K best columns: \", selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # random forest\n",
    "# random_forest_model(tags=['random forest', 'k=20'], feature_set=selected_cols, drop=False)\n",
    "\n",
    "# # elatic net regr\n",
    "# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=selected_cols, drop=False)\n",
    "\n",
    "# # GBR\n",
    "# gbr_model(tags=['gradient boosting regressor'], feature_set=selected_cols, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "Recursive feature selection works by eliminating the least important features. It continues recursively until the specified number of features is reached. Recursive elimination can be used with any model that assigns weights to features, either through `coef_` or `feature_importances_`. Here we will use random forests to select the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Experiment: \n",
      "created new ExperimentRun: \n",
      "Support:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True False  True  True  True  True False  True  True  True False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True]\n",
      "Ranking:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 1 3 1 1 1 1 4 1 1 1 2 5 1 2 2 2 6 2 3 3 3 7 3 3 3 3\n",
      " 8 4 4 4 4 4 4 5 5 5 5 5 5 6 6 6 6 6 6 6 8 8 8 8 8 8 8 8 7 8 7 7 7 7 7 7 7\n",
      " 7 6 6 5 5 5 4 4 4 3 3 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1]\n",
      "Selected Features:  ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'installment', 'emp_length', 'annual_inc', 'issue_d', 'zip_code', 'dti', 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'last_credit_pull_d', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc', 'mths_since_recent_inq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'term_ 36 months', 'addr_state_NJ', 'addr_state_NM', 'addr_state_NV', 'addr_state_NY', 'addr_state_OH', 'addr_state_OK', 'addr_state_OR', 'addr_state_PA', 'addr_state_RI', 'addr_state_SC', 'addr_state_SD', 'addr_state_TN', 'addr_state_TX', 'addr_state_UT', 'addr_state_VA', 'addr_state_VT', 'addr_state_WA', 'addr_state_WI', 'addr_state_WV', 'addr_state_WY', 'application_type_Individual', 'application_type_Joint App', 'disbursement_method_Cash', 'disbursement_method_DirectPay']\n",
      "Getting metrics for model\n",
      "train mae:  3.0752  test mae:  3.0734\n",
      "train rmse:  3.0752  test rmse:  3.0734\n"
     ]
    }
   ],
   "source": [
    "# RFE - Feature ranking with recursive feature elimination.\n",
    "expt = client.set_experiment(\"RFE\")\n",
    "\n",
    "# run\n",
    "run_rfe = client.set_experiment_run()\n",
    "\n",
    "# n_features_to_select can be increased or decreased, 80 here is just an example\n",
    "hyperparam_rfe = {\"step\":10, \"n_features_to_select\":80}\n",
    "hyperparam_rfr = {\"n_estimators\":20, \"max_depth\":4}\n",
    "\n",
    "estimator = RandomForestRegressor(random_state = 42, n_jobs=-1, **hyperparam_rfr)\n",
    "selector = RFE(estimator, **hyperparam_rfe)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "print(\"Support: \", selector.support_)\n",
    "run_rfe.log_artifact(\"support\", selector.support_)\n",
    "print(\"Ranking: \", selector.ranking_)\n",
    "run_rfe.log_artifact(\"ranking\", selector.ranking_)\n",
    "selected_cols = [d for d, s in zip(list(X_train.columns), selector.support_) if s]\n",
    "print(\"Selected Features: \", selected_cols)\n",
    "run_rfe.log_artifact(\"feature_set\", selected_cols)\n",
    "\n",
    "run_rfe.log_hyperparameters(hyperparam_rfe)\n",
    "run_rfe.log_hyperparameters(hyperparam_rfr)\n",
    "\n",
    "run_rfe.log_tags(['random forest'])\n",
    "\n",
    "train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train,\n",
    "                                                         y_train = y_train,\n",
    "                                                         X_test = X_test,\n",
    "                                                         y_test = y_test,\n",
    "                                                         model = selector,\n",
    "                                                         run = run_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded methods \n",
    "\n",
    "\n",
    "Embedded methods combine the feature selection and the learning algorithm. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously.\n",
    "\n",
    "#### Ridge\n",
    "Ridge is another way to do feature selection, where we penalize the coefficients of the model for being too large. This is done by adding a penalty or shrinkage estimator to the regression, which can be controlled with the term *lambda*. Higher the value of lambda, more the shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Experiment: \n",
      "created new ExperimentRun: \n",
      "{'alpha': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=7.25277e-22): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metrics for model\n",
      "train mae:  2.1616  test mae:  2.1636\n",
      "train rmse:  2.1616  test rmse:  2.1636\n",
      "train_mae, test_mae, train_rmse, test_rmse -  2.1616 2.1636 2.812357521884802 2.8142122626638364\n"
     ]
    }
   ],
   "source": [
    "expt = client.set_experiment(\"ridge\")\n",
    "# Ridge\n",
    "# increase the num in np.logspace() to search over a larger set of alpha values\n",
    "hyperparam_candidates = {\n",
    "    'alpha' : np.logspace(-4, -1, 1)\n",
    "}\n",
    "hyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n",
    "                   for values\n",
    "                   in itertools.product(*hyperparam_candidates.values())]\n",
    "\n",
    "def run_experiment(hyperparams, **params):\n",
    "    \n",
    "    # create object to track experiment run\n",
    "    run = client.set_experiment_run()\n",
    "    run.log_tags(['ridge', 'regularization'])\n",
    "    \n",
    "    # log hyperparameters\n",
    "    run.log_hyperparameters(hyperparams)\n",
    "    print(hyperparams)\n",
    "    \n",
    "    # create and train model\n",
    "    ridge_model = Ridge(**hyperparams)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    \n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train, y_train, X_test, y_test, ridge_model, run)\n",
    "    print(\"train_mae, test_mae, train_rmse, test_rmse - \", train_mae, test_mae, train_rmse, test_rmse)\n",
    "    \n",
    "with Pool() as pool:\n",
    "    pool.map(run_experiment, hyperparam_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso\n",
    "Lasso Regression can be used to penalize the beta coefficients in a model, and it is very similar to ridge regression. It also adds a penalty term to the cost function of a model, with a *lambda* value that must be tuned. The most important distinction from ridge regression is that Lasso regression can force the Beta coefficient to zero, which will remove that feature from the model. This is why Lasso is preferred at times, especially when you are looking to reduce model complexity. The smaller number of features a model has, the lower the complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "scaled_x_train = pd.DataFrame(ss.fit_transform(X_train),columns = X_train.columns)\n",
    "print(scaled_x_train.shape == X_train.shape)\n",
    "scaled_x_test = pd.DataFrame(ss.fit_transform(X_test),columns = X_test.columns)\n",
    "print(scaled_x_test.shape == X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Experiment: \n",
      "created new ExperimentRun: \n",
      "{'alpha': 0.0001}\n",
      "Getting metrics for model\n",
      "train mae:  2.1617  test mae:  2.1627\n",
      "train rmse:  2.1617  test rmse:  2.1627\n",
      "train_mae, test_mae, train_rmse, test_rmse -  2.1617 2.1627 2.81238059632093 2.8138177594439293\n"
     ]
    }
   ],
   "source": [
    "expt = client.set_experiment(\"Lasso\")\n",
    "# Lasso\n",
    "# increase the num in np.logspace() to search over a larger set of alpha values\n",
    "hyperparam_candidates = {\n",
    "    'alpha' : np.logspace(-4, -1, 1)\n",
    "}\n",
    "\n",
    "hyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n",
    "                   for values\n",
    "                   in itertools.product(*hyperparam_candidates.values())]\n",
    "\n",
    "def run_experiment(hyperparams):\n",
    "    \n",
    "    # create object to track experiment run\n",
    "    run = client.set_experiment_run()\n",
    "    run.log_tags(['lasso', 'regularization', 'scaled'])\n",
    "    \n",
    "    # log hyperparameters\n",
    "    run.log_hyperparameters(hyperparams)\n",
    "    print(hyperparams)\n",
    "    # create and train model\n",
    "    model = Lasso(alpha=hyperparams['alpha'], max_iter = 1e5, tol=0.001)\n",
    "    model.fit(scaled_x_train, y_train)\n",
    "    \n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(scaled_x_train, y_train, scaled_x_test, y_test, model, run)\n",
    "    print(\"train_mae, test_mae, train_rmse, test_rmse - \", train_mae, test_mae, train_rmse, test_rmse)\n",
    "    \n",
    "with Pool() as pool:\n",
    "    pool.map(run_experiment, hyperparam_sets)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up!\n",
    "\n",
    "Overall we have seen a number of different ways to do feature selection and we have constructed about 150 different models to see how each of them perform for this dataset. Of these the best methods turned out to be:\n",
    "\n",
    "* Use Lasso Regression with scaled data:\n",
    "    Hyperparameters: `{ alpha=0.0001 ; max_iter=1000; tol=0.0001 }`\n",
    "* Gradient Boosting Regressor with SelectKBest:\n",
    "    Hyperparameters: `{learning_rate=0.1, n_estimators=100, min_samples_leaf=3, max_depth=9}`\n",
    "    \n",
    "Here is a boxplot which summarizes how well each feature selection method does - \n",
    "![image.png](attachment:image.png)\n",
    "![image.png](attachment:image.png)\n",
    "From the above plots we see that regularization methods work well for this dataset and can be a great starting point. However if you have more compute at your disposal, combining this feature selection method with a gradient boosting would give you much higher accuracies. \n",
    "\n",
    "Here are some useful links if you would like to learn more -\n",
    "* [Feature Selection: A survey](http://www.realtechsupport.org/UB/ML+CT/papers/Sahin_FeatureSelectionMethods_2014.pdf)\n",
    "* [LendingHome Dataset](https://www.kaggle.com/wendykan/lending-club-loan-data) \n",
    "* Verta AI:\n",
    "  * Want to use Verta in your experiments? [Start here](https://verta.readme.io/docs/getting-started)\n",
    "  * [Why do we need model versioning?](https://medium.com/vertaai/how-to-move-fast-in-ai-without-breaking-things-3ecb74eafd18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
